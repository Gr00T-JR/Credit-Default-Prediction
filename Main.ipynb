{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64eab4c",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction Project\n",
    "\n",
    "Based on the dataset UCI Machine Learning Repository\n",
    "\n",
    "The original paper that works with this dataset is : Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n",
    "\n",
    "* __[Link to original paper](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf)__\n",
    "\n",
    "* __[Link to UCI dataset page](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)__\n",
    "\n",
    "### Dataset Description\n",
    "* Data consists of 30 000 points and 23 features and 1 label\n",
    "\n",
    "\n",
    "### Project Outline\n",
    "Data preparation and exploration -> ML models hyperparameters tuning -> Combination into a final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01f931",
   "metadata": {},
   "source": [
    "## Import : Data and Libraries\n",
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02c9f5d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T21:00:38.822020Z",
     "start_time": "2021-01-06T21:00:36.808982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Optimizer\n",
    "from hyperopt import Trials, fmin, tpe\n",
    "\n",
    "from EvalModel_helper import fit_Model, EvalMetrics\n",
    "\n",
    "\n",
    "# Model hyperparameter space to be optimized\n",
    "\n",
    "## siwtch order!\n",
    "from Models_spaces import xgb_fitting_setting, xgb_int_keys, xgb_space, ada_loss_functions, ada_space,\\\n",
    "    gbrt_loss_functions, gbrt_int_keys, gbrt_space, log_space, log_penalties, log_solvers,\\\n",
    "    svm_space, svm_kernels, svm_kernel_degrees\n",
    "\n",
    "# Model objective function\n",
    "from Hyperopt_objective import build_objective_func\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e17a3",
   "metadata": {},
   "source": [
    "\n",
    "### Import and pre-processing of dataset \n",
    "(preprocessing : transforming data into ML model readable format)\n",
    "\n",
    "#### Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "307f34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting train_file and test_file for import\n",
    "### EDIT FILEPATH IF NECESSARY\n",
    "root = '.'\n",
    "data_dir = '/DataFiles/'\n",
    "\n",
    "# form filepaths\n",
    "data_path = root + data_dir\n",
    "train_file = data_path + 'CreditCard_train.csv'\n",
    "test_file = data_path + 'CreditCard_test.csv'\n",
    "\n",
    "# load\n",
    "_df_train = pd.read_csv(train_file, index_col=0, header=1)\n",
    "_df_test = pd.read_csv(test_file, index_col=0, header=1)\n",
    "\n",
    "# create copy df for handling\n",
    "df_train = _df_train.copy()\n",
    "df_test = _df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc5862",
   "metadata": {},
   "source": [
    "#### Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "    LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\nID                                                                         \n1       20000    2          2         1   24      2      2     -1     -1   \n2      120000    2          2         2   26     -1      2      0      0   \n3       90000    2          2         2   34      0      0      0      0   \n4       50000    2          2         1   37      0      0      0      0   \n5       50000    1          2         1   57     -1      0     -1      0   \n\n    PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\nID         ...                                                                  \n1      -2  ...          0          0          0         0       689         0   \n2       0  ...       3272       3455       3261         0      1000      1000   \n3       0  ...      14331      14948      15549      1518      1500      1000   \n4       0  ...      28314      28959      29547      2000      2019      1200   \n5       0  ...      20940      19146      19131      2000     36681     10000   \n\n    PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \nID                                                            \n1          0         0         0                           1  \n2       1000         0      2000                           1  \n3       1000      1000      5000                           0  \n4       1100      1069      1000                           0  \n5       9000       689       679                           0  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LIMIT_BAL</th>\n      <th>SEX</th>\n      <th>EDUCATION</th>\n      <th>MARRIAGE</th>\n      <th>AGE</th>\n      <th>PAY_0</th>\n      <th>PAY_2</th>\n      <th>PAY_3</th>\n      <th>PAY_4</th>\n      <th>PAY_5</th>\n      <th>...</th>\n      <th>BILL_AMT4</th>\n      <th>BILL_AMT5</th>\n      <th>BILL_AMT6</th>\n      <th>PAY_AMT1</th>\n      <th>PAY_AMT2</th>\n      <th>PAY_AMT3</th>\n      <th>PAY_AMT4</th>\n      <th>PAY_AMT5</th>\n      <th>PAY_AMT6</th>\n      <th>default payment next month</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>20000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>689</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>120000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>26</td>\n      <td>-1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3272</td>\n      <td>3455</td>\n      <td>3261</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>14331</td>\n      <td>14948</td>\n      <td>15549</td>\n      <td>1518</td>\n      <td>1500</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>5000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>28314</td>\n      <td>28959</td>\n      <td>29547</td>\n      <td>2000</td>\n      <td>2019</td>\n      <td>1200</td>\n      <td>1100</td>\n      <td>1069</td>\n      <td>1000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>57</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>20940</td>\n      <td>19146</td>\n      <td>19131</td>\n      <td>2000</td>\n      <td>36681</td>\n      <td>10000</td>\n      <td>9000</td>\n      <td>689</td>\n      <td>679</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ed37f0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            LIMIT_BAL           SEX     EDUCATION     MARRIAGE           AGE  \\\ncount    24000.000000  24000.000000  24000.000000  24000.00000  24000.000000   \nmean    165495.986667      1.628250      1.847417      1.55725     35.380458   \nstd     129128.744855      0.483282      0.780007      0.52208      9.271050   \nmin      10000.000000      1.000000      0.000000      0.00000     21.000000   \n25%      50000.000000      1.000000      1.000000      1.00000     28.000000   \n50%     140000.000000      2.000000      2.000000      2.00000     34.000000   \n75%     240000.000000      2.000000      2.000000      2.00000     41.000000   \nmax    1000000.000000      2.000000      6.000000      3.00000     79.000000   \n\n              PAY_0        PAY_2         PAY_3         PAY_4         PAY_5  \\\ncount  24000.000000  24000.00000  24000.000000  24000.000000  24000.000000   \nmean      -0.003125     -0.12350     -0.154750     -0.211667     -0.252917   \nstd        1.123425      1.20058      1.204033      1.166549      1.136993   \nmin       -2.000000     -2.00000     -2.000000     -2.000000     -2.000000   \n25%       -1.000000     -1.00000     -1.000000     -1.000000     -1.000000   \n50%        0.000000      0.00000      0.000000      0.000000      0.000000   \n75%        0.000000      0.00000      0.000000      0.000000      0.000000   \nmax        8.000000      8.00000      8.000000      8.000000      8.000000   \n\n       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\ncount  ...   24000.000000   24000.000000   24000.000000   24000.000000   \nmean   ...   42368.188417   40000.682542   38563.710625    5542.912917   \nstd    ...   63070.680934   60345.012766   59155.759799   15068.576072   \nmin    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n25%    ...    2340.000000    1740.000000    1234.750000    1000.000000   \n50%    ...   18940.500000   18107.500000   17036.000000    2100.000000   \n75%    ...   52188.500000   49746.500000   48796.250000    5000.000000   \nmax    ...  891586.000000  927171.000000  961664.000000  505000.000000   \n\n           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\ncount  2.400000e+04   24000.00000   24000.000000   24000.000000   \nmean   5.815336e+03    4969.26600    4743.480042    4783.486042   \nstd    2.079703e+04   16095.61434   14883.269990   15270.405279   \nmin    0.000000e+00       0.00000       0.000000       0.000000   \n25%    8.000000e+02     379.00000     279.750000     244.000000   \n50%    2.000000e+03    1702.50000    1500.000000    1500.000000   \n75%    5.000000e+03    4347.25000    4000.000000    4005.000000   \nmax    1.684259e+06  896040.00000  497000.000000  417990.000000   \n\n            PAY_AMT6  default payment next month  \ncount   24000.000000                24000.000000  \nmean     5189.399042                    0.223750  \nstd     17630.371990                    0.416765  \nmin         0.000000                    0.000000  \n25%        60.750000                    0.000000  \n50%      1500.000000                    0.000000  \n75%      4000.000000                    0.000000  \nmax    528666.000000                    1.000000  \n\n[8 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LIMIT_BAL</th>\n      <th>SEX</th>\n      <th>EDUCATION</th>\n      <th>MARRIAGE</th>\n      <th>AGE</th>\n      <th>PAY_0</th>\n      <th>PAY_2</th>\n      <th>PAY_3</th>\n      <th>PAY_4</th>\n      <th>PAY_5</th>\n      <th>...</th>\n      <th>BILL_AMT4</th>\n      <th>BILL_AMT5</th>\n      <th>BILL_AMT6</th>\n      <th>PAY_AMT1</th>\n      <th>PAY_AMT2</th>\n      <th>PAY_AMT3</th>\n      <th>PAY_AMT4</th>\n      <th>PAY_AMT5</th>\n      <th>PAY_AMT6</th>\n      <th>default payment next month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.00000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.00000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>...</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>2.400000e+04</td>\n      <td>24000.00000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n      <td>24000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>165495.986667</td>\n      <td>1.628250</td>\n      <td>1.847417</td>\n      <td>1.55725</td>\n      <td>35.380458</td>\n      <td>-0.003125</td>\n      <td>-0.12350</td>\n      <td>-0.154750</td>\n      <td>-0.211667</td>\n      <td>-0.252917</td>\n      <td>...</td>\n      <td>42368.188417</td>\n      <td>40000.682542</td>\n      <td>38563.710625</td>\n      <td>5542.912917</td>\n      <td>5.815336e+03</td>\n      <td>4969.26600</td>\n      <td>4743.480042</td>\n      <td>4783.486042</td>\n      <td>5189.399042</td>\n      <td>0.223750</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>129128.744855</td>\n      <td>0.483282</td>\n      <td>0.780007</td>\n      <td>0.52208</td>\n      <td>9.271050</td>\n      <td>1.123425</td>\n      <td>1.20058</td>\n      <td>1.204033</td>\n      <td>1.166549</td>\n      <td>1.136993</td>\n      <td>...</td>\n      <td>63070.680934</td>\n      <td>60345.012766</td>\n      <td>59155.759799</td>\n      <td>15068.576072</td>\n      <td>2.079703e+04</td>\n      <td>16095.61434</td>\n      <td>14883.269990</td>\n      <td>15270.405279</td>\n      <td>17630.371990</td>\n      <td>0.416765</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>10000.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>21.000000</td>\n      <td>-2.000000</td>\n      <td>-2.00000</td>\n      <td>-2.000000</td>\n      <td>-2.000000</td>\n      <td>-2.000000</td>\n      <td>...</td>\n      <td>-170000.000000</td>\n      <td>-81334.000000</td>\n      <td>-339603.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>50000.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>28.000000</td>\n      <td>-1.000000</td>\n      <td>-1.00000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>...</td>\n      <td>2340.000000</td>\n      <td>1740.000000</td>\n      <td>1234.750000</td>\n      <td>1000.000000</td>\n      <td>8.000000e+02</td>\n      <td>379.00000</td>\n      <td>279.750000</td>\n      <td>244.000000</td>\n      <td>60.750000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>140000.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>34.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>18940.500000</td>\n      <td>18107.500000</td>\n      <td>17036.000000</td>\n      <td>2100.000000</td>\n      <td>2.000000e+03</td>\n      <td>1702.50000</td>\n      <td>1500.000000</td>\n      <td>1500.000000</td>\n      <td>1500.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>240000.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>41.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>52188.500000</td>\n      <td>49746.500000</td>\n      <td>48796.250000</td>\n      <td>5000.000000</td>\n      <td>5.000000e+03</td>\n      <td>4347.25000</td>\n      <td>4000.000000</td>\n      <td>4005.000000</td>\n      <td>4000.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1000000.000000</td>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>3.00000</td>\n      <td>79.000000</td>\n      <td>8.000000</td>\n      <td>8.00000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n      <td>...</td>\n      <td>891586.000000</td>\n      <td>927171.000000</td>\n      <td>961664.000000</td>\n      <td>505000.000000</td>\n      <td>1.684259e+06</td>\n      <td>896040.00000</td>\n      <td>497000.000000</td>\n      <td>417990.000000</td>\n      <td>528666.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 24 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transforming the dataframe into training set and test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "118dbadd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# renaming columns for consistency and simplicity\n",
    "df_train = df_train.rename(columns={'PAY_0':'PAY_1', 'default payment next month':'DEFAULT'})\n",
    "df_test = df_test.rename(columns={'PAY_0':'PAY_1', 'default payment next month':'DEFAULT'})\n",
    "\n",
    "label = df_train.columns[-1] # = `DEFAULT`\n",
    "features = list(df_train.columns)[:-1]\n",
    "\n",
    "y_train = df_train[label]\n",
    "X_train = df_train[features]\n",
    "\n",
    "y_test = df_test[label]\n",
    "X_test = df_test[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b9cb1",
   "metadata": {},
   "source": [
    "__Comment__ : All the data types are integers and thus workable for ML models. There are no null values (arbitrarily checked and all features have the same count). Values in the `SEX`, `EDUCATION` are have a specified range, however, some values are not contained. <br>\n",
    "Optional to check and explore the data further into Data_Exploration."
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump([X_train, y_train], f)\n",
    "f.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "53b71d61",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Pipeline\n",
    "* includes scaling, sampling and (future work : feature transformation)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "179757df",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Dysfunctional code\n",
    "method0 = Pipeline([\n",
    "    ('sampler', SMOTE(random_state=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We take the training:validation:test ratios as 60:20:20.\n",
    "We change X_train into a training set and validation set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a2a0496",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling the dataset for computational efficiency of ML models operations (fitting, prediction)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# X_train_transformed, y_train_transformed = SMOTE(random_state=3).fit_resample(X=X_train, y=y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "73a57eb8",
   "metadata": {},
   "source": [
    "### Benchmarking some standard ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73634e38",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Checking the ML models\n",
    "* xgboost, adaboost, gradientboostingregressor, logistic regression and support vector machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9031e35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameter tuning of ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving data in a pickle file and opened again in model objectives. (not sure if I can include them as input to the objectives of each model, for version 2..)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "with open('objs.pkl', 'wb') as f:\n",
    "    pickle.dump([X_train, y_train, X_validation, y_validation], f)\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "17b3f108",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hyperparameter tuning framework consists of a tuner (hyperopt), optimization space (model dependent), and objective function (model  dependent)\n",
    "These are imported.\n",
    "\n",
    "### ML models to be optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a580e263",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'xgb_int_keys' from 'Models_spaces' (/Users/Jurren/Desktop/GitHub/Credit-Default-Prediction/Models_spaces.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-c19dd51c7be6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Model hyperparameter space to be optimized\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mModels_spaces\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mxgb_space\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxgb_int_keys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxgb_fitting_setting\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mada_space\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mada_loss_functions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0mgbrt_space\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgbrt_loss_functions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgbrt_int_keys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_space\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_penalties\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_solvers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0msvm_space\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msvm_kernels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msvm_kernel_degrees\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'xgb_int_keys' from 'Models_spaces' (/Users/Jurren/Desktop/GitHub/Credit-Default-Prediction/Models_spaces.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4faa55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tuning\n",
    "\n",
    "For tuning we will be first split up the training data into a validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23e13a",
   "metadata": {},
   "source": [
    "long_run functionality, to check the script we can have the variable `short_run=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_run = False\n",
    "\n",
    "if long_run:\n",
    "    max_evals = 50\n",
    "    max_evals_gbrt = 7\n",
    "if not long_run:\n",
    "    max_evals = 1\n",
    "    max_evals_gbrt = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b59101",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "\n",
    "xgb_objective = build_objective_func(model=xgb.XGBRegressor, X_train=X_train, y_train=y_train, cross_val_method=0, int_keys=xgb_int_keys, **xgb_fitting_setting)\n",
    "xgb_best_hyperparams = fmin(fn = xgb_objective,\n",
    "                        space = xgb_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = max_evals,\n",
    "                        trials = trials)\n",
    "\n",
    "for key in xgb_int_keys:\n",
    "    xgb_best_hyperparams[key] = int(xgb_best_hyperparams[key])\n",
    "\n",
    "print(\"The XGBRegressor hyperparameters found are : \",\"\\n\")\n",
    "print(xgb_best_hyperparams)\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(silent=False, **xgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "id": "62ed627c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "# still to be changed to similar to following models\n",
    "xgb_reg = fit_Model(xgb_reg, X_train, y_train,eval_metrics=True, confusion_matrix=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157d55f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "ada_objective = build_objective_func(model=AdaBoostRegressor, X_train=X_train, y_train=y_train,\n",
    "                                      cross_val_method=0)\n",
    "\n",
    "ada_best_hyperparams = fmin(fn = ada_objective,\n",
    "                        space = ada_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = max_evals,\n",
    "                        trials = trials)\n",
    "\n",
    "# `ada_space` references the index of the ada_loss_functions, not the name\n",
    "ada_best_hyperparams['loss'] = ada_loss_functions[ada_best_hyperparams['loss']]\n",
    "\n",
    "print(\"The AdaBoostRegressor hyperparameters found are : \",\"\\n\")\n",
    "print(ada_best_hyperparams)\n",
    "\n",
    "ada_reg = AdaBoostRegressor(**ada_best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1427409",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "ada_reg = fit_Model(ada_reg, X_train, y_train,eval_metrics=True, confusion_matrix=True)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "trials = Trials()\n",
    "\n",
    "gbrt_objective = build_objective_func(model=GradientBoostingRegressor, X_train=X_train, y_train=y_train,\n",
    "                                      cross_val_method=0, int_keys=gbrt_int_keys)\n",
    "\n",
    "gbrt_best_hyperparams = fmin(fn = gbrt_objective,\n",
    "                        space = gbrt_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = max_evals_gbrt,\n",
    "                        trials = trials)\n",
    "\n",
    "for key in gbrt_int_keys:\n",
    "    gbrt_best_hyperparams[key] = int(gbrt_best_hyperparams[key])\n",
    "\n",
    "gbrt_best_hyperparams['loss'] = gbrt_loss_functions[gbrt_best_hyperparams['loss']]\n",
    "\n",
    "print(\"The GradientBoostingRegressor hyperparameters found are : \",\"\\n\")\n",
    "print(gbrt_best_hyperparams)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbrt_reg = GradientBoostingRegressor(**gbrt_best_hyperparams)\n",
    "\n",
    "gbrt_reg = fit_Model(gbrt_reg, X_train, y_train,eval_metrics=True, confusion_matrix=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd22aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "log_objective = build_objective_func(model=LogisticRegression, X_train=X_train, y_train=y_train,\n",
    "                                      cross_val_method=0)\n",
    "\n",
    "\n",
    "log_best_hyperparams = fmin(fn = log_objective,\n",
    "                        space = log_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = max_evals,\n",
    "                        trials = trials)\n",
    "\n",
    "log_best_hyperparams['penalty'] = log_penalties[log_best_hyperparams['penalty']]\n",
    "log_best_hyperparams['solver'] = log_solvers[log_best_hyperparams['solver']]\n",
    "\n",
    "\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(log_best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021af32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_reg = LogisticRegression(**log_best_hyperparams)\n",
    "\n",
    "log_reg = fit_Model(log_reg, X_train, y_train,eval_metrics=True, confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e87f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "svm_objective = build_objective_func(model=SVC, X_train=X_train, y_train=y_train,\n",
    "                                      cross_val_method=0)\n",
    "\n",
    "svm_best_hyperparams = fmin(fn = svm_objective,\n",
    "                        space = svm_space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = max_evals,\n",
    "                        trials = trials)\n",
    "\n",
    "svm_best_hyperparams['kernel'] = svm_kernels[svm_best_hyperparams['kernel']]\n",
    "if svm_best_hyperparams['kernel'] != 'poly':\n",
    "    svm_best_hyperparams.pop('degree')\n",
    "else:\n",
    "    svm_best_hyperparams['degree'] = svm_kernel_degrees[svm_best_hyperparams['degree']]\n",
    "\n",
    "\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(svm_best_hyperparams)"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "%run EvalModel_file.ipynb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'EvalModel_file.ipynb.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/magics/execution.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    702\u001B[0m             \u001B[0mfpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marg_lst\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 703\u001B[0;31m             \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfile_finder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    704\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mIndexError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/utils/path.py\u001B[0m in \u001B[0;36mget_py_filename\u001B[0;34m(name, force_win32)\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 109\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'File `%r` not found.'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: File `'EvalModel_file.ipynb.py'` not found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-489b979604b8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'run'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'EvalModel_file.ipynb'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mrun_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2346\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'local_ns'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_local_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstack_depth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2347\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuiltin_trap\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2348\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2349\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2350\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/decorator.py\u001B[0m in \u001B[0;36mfun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mkwsyntax\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    231\u001B[0m                 \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 232\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcaller\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mextras\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    233\u001B[0m     \u001B[0mfun\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    234\u001B[0m     \u001B[0mfun\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/magic.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(f, *a, **k)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[0;31m# but it's overkill for just that one bit of state.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mmagic_deco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m         \u001B[0mcall\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    188\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/IPython/core/magics/execution.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    712\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'nt'\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr\"^'.*'$\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    713\u001B[0m                 \u001B[0mwarn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 714\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    715\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    716\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mfpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeta_path\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mException\u001B[0m: File `'EvalModel_file.ipynb.py'` not found."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab39f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_reg = SVC(**svm_best_hyperparams)\n",
    "\n",
    "svm_reg = fit_Model(svm_reg, X_train, y_train,eval_metrics=True, confusion_matrix=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df000aaa",
   "metadata": {},
   "source": [
    "The search selects the best considered (not the best in the space) generalizable hyperparameters (i.e. the ones that perform best, after fitting on the training set and prediction on the validation set).\n",
    "In this limited analysis, these hyperparameters are considered the best generalizable.\n",
    "Alternatively, we can say that we select these parameters to be tested.\n",
    "We may find that some ML models allow for a large fluctuation in performance on the validation set, which may indicate that we have 'overfitted' the validation set. This will show on the test set.\n",
    "We can decide to further train on the validation set, however, it is interesting to see how the performance of the model changes whether we use the validation set or not.\n",
    "\n",
    "The models are:\n",
    "`xgb_reg`, `ada_reg`, `gbrt_reg`, `log_reg`, and `svm_reg`\n",
    "\n",
    "Now we can test these models on the test set. Followed by checking each prediction's 'strenght' by using the Smooth Sorting Method as proposed in the original paper by Yeh and Lien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing these models\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from EvalModel_helper import EvalMetrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd748eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xgb_reg, ada_reg, gbrt_reg, log_reg, svm_reg]\n",
    "for model in models:\n",
    "    y_predicted = model.predict(X_test)>0.5\n",
    "    print(f'Performance on test data of model {model} \\n')\n",
    "    EvalMetrics(y_test, y_predicted, confusion_matrix = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We find that these\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "168c3afe",
   "metadata": {},
   "source": [
    "## Performance at given percentages\n",
    "### robustness\n",
    "\n",
    "As opposed to simply classifiying clients as expected to default vs not-expected to default, quantifying is more meaningful. I.e. defining a probability of default has more potential.\n",
    "\n",
    "To estimate the real probability, the Smooth Sorting Method can be used, which estimates the real probability by looking at neighboring points and taking the mean of these values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbabd6",
   "metadata": {},
   "source": [
    "__Smooth Sorting Method__ from the original paper (Yeh, I. C., & Lien, C. H. (2009)): \n",
    "\n",
    "$$\\text{P}_i = \\frac{\\sum_{j=-n}^{n}\\text{Y}_{i-j}}{2n+1}$$\n",
    "\n",
    "where $\\text{P}_i$ is the estimated real probability of default, $\\text{Y}_{i}$ is the binary variable of default (1) or non-default (0), $n$ is the number of data for smoothing.<br>\n",
    "The Smooth Sorting Method is used on sorted data, from the lowest probability of default occuring to the highest probability of default occuring. \n",
    "\n",
    "This is interesting to look at because loaners adopt different risk strategies.    \n",
    "( (?) for this we consider at 20% and 80% (?) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec14761",
   "metadata": {},
   "source": [
    "we have the lists : `y_predicted` and `y_test`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9551f14",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "xgb_reg = xgb.XGBRegressor(eta=0.3, gamma=0.5, use_label_encoder=False)\n",
    "xgb_reg.fit(X_train,y_train)\n",
    "y_predicted = xgb_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44737b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709565ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = np.argsort(y_predicted)\n",
    "\n",
    "y_test = y_test[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_avg = []\n",
    "n = 50\n",
    "for counter in range(n,len(y_test_sorted)-n):\n",
    "    intermediate_val = np.mean(y_test_sorted[counter-n:counter+n])\n",
    "    y_avg.append(intermediate_val)\n",
    "    \n",
    "y_predicted_sorted = sorted(y_predicted[n:len(y_predicted)-n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4507880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(y_predicted[n:len(y_predicted)-n]),y_avg)\n",
    "plt.grid(True)\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8638e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_predicted[n:len(y_predicted)-n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee291ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca67e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbedc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_selected = y_predicted[n:len(y_predicted)-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6610a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(r2_score(y_avg,y_predicted_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c73b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(y_predicted)-n\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (CW2)",
   "language": "python",
   "name": "pycharm-1482e948"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}