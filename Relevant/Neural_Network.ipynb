{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from nn_model import build_model\n",
    "\n",
    "%run Data_ImportAndExplore.ipynb\n",
    "X_train_, X_validation, y_train_, y_validation = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Optimization of NN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SPACE = {\n",
    "    'hidden_size': hp.quniform('hidden_size', low=300, high=2400, q=100),\n",
    "    'depth': hp.quniform('depth', low=2, high=6, q=1),\n",
    "    'dropout': hp.quniform('dropout', low=0.0, high=0.4, q=0.05),\n",
    "    'ffn_num_layers': hp.quniform('ffn_num_layers', low=1, high=3, q=1)\n",
    "}\n",
    "INT_KEYS = ['hidden_size', 'depth', 'ffn_num_layers']\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    clf=(\n",
    "                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree']))\n",
    "\n",
    "    evaluation = [( X_train_, y_train_), ( X_validation, y_validation)]\n",
    "\n",
    "    clf.fit(X_train_, y_train_,\n",
    "            eval_set=evaluation, eval_metric=\"auc\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "\n",
    "\n",
    "    pred = clf.predict(X_validation)\n",
    "    accuracy = accuracy_score(y_validation, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "def grid_search(args: Namespace):\n",
    "\n",
    "    # Run grid search\n",
    "    results = []\n",
    "\n",
    "    # Define hyperparameter optimization\n",
    "    def objective(hyperparams: Dict[str, Union[int, float]]) -> float:\n",
    "        # Convert hyperparams from float to int when necessary\n",
    "        for key in INT_KEYS:\n",
    "            hyperparams[key] = int(hyperparams[key])\n",
    "\n",
    "        # Update args with hyperparams\n",
    "        hyper_args = deepcopy(args)\n",
    "        if args.save_dir is not None:\n",
    "            folder_name = '_'.join(f'{key}_{value}' for key, value in hyperparams.items())\n",
    "            hyper_args.save_dir = os.path.join(hyper_args.save_dir, folder_name)\n",
    "        for key, value in hyperparams.items():\n",
    "            setattr(hyper_args, key, value)\n",
    "\n",
    "        # Record hyperparameters\n",
    "        logger.info(hyperparams)\n",
    "\n",
    "        # Cross validate\n",
    "        mean_score, std_score = cross_validate(hyper_args, train_logger)\n",
    "\n",
    "        # Record results\n",
    "        temp_model = build_model(hyper_args)\n",
    "        num_params = param_count(temp_model)\n",
    "        logger.info(f'num params: {num_params:,}')\n",
    "        logger.info(f'{mean_score} +/- {std_score} {hyper_args.metric}')\n",
    "\n",
    "        results.append({\n",
    "            'mean_score': mean_score,\n",
    "            'std_score': std_score,\n",
    "            'hyperparams': hyperparams,\n",
    "            'num_params': num_params\n",
    "        })\n",
    "\n",
    "        # Deal with nan\n",
    "        if np.isnan(mean_score):\n",
    "            if hyper_args.dataset_type == 'classification':\n",
    "                mean_score = 0\n",
    "            else:\n",
    "                raise ValueError('Can\\'t handle nan score for non-classification dataset.')\n",
    "\n",
    "        return (1 if hyper_args.minimize_score else -1) * mean_score\n",
    "\n",
    "    fmin(objective, SPACE, algo=tpe.suggest, max_evals=args.num_iters)\n",
    "\n",
    "    # Report best result\n",
    "    results = [result for result in results if not np.isnan(result['mean_score'])]\n",
    "    best_result = min(results, key=lambda result: (1 if args.minimize_score else -1) * result['mean_score'])\n",
    "    logger.info('best')\n",
    "    logger.info(best_result['hyperparams'])\n",
    "    logger.info(f'num params: {best_result[\"num_params\"]:,}')\n",
    "    logger.info(f'{best_result[\"mean_score\"]} +/- {best_result[\"std_score\"]} {args.metric}')\n",
    "\n",
    "    # Save best hyperparameter settings as JSON config file\n",
    "    makedirs(args.config_save_path, isfile=True)\n",
    "\n",
    "    with open(args.config_save_path, 'w') as f:\n",
    "        json.dump(best_result['hyperparams'], f, indent=4, sort_keys=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 50,\n",
    "                        trials = trials)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}