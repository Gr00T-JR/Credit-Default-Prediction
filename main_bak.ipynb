{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Credit Card Default Prediction Project\n",
    "\n",
    "Based on the dataset UCI Machine Learning Repository\n",
    "\n",
    "The original paper that works with this dataset is : Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n",
    "\n",
    "* __[Link to original paper](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf)__\n",
    "\n",
    "* __[Link to UCI dataset page](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)__\n",
    "\n",
    "### Dataset Description\n",
    "* Data consists of 30 000 points and 23 features and 1 label\n",
    "\n",
    "\n",
    "### Project Outline\n",
    "Data preparation and exploration -> ML models hyperparameters tuning -> Combination into a final model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import : Data and Libraries\n",
    "### Library Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, r2_score\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Optimizer\n",
    "from hyperopt import Trials, fmin, tpe\n",
    "\n",
    "from evaluation_helper import performance_metrics\n",
    "\n",
    "\n",
    "# Model hyperparameter space to be optimized\n",
    "from hyperparameters_spaces import ada_loss_functions, ada_space, log_space, svm_space, svm_kernels, \\\n",
    "    svm_kernel_degrees, xgb_space, INT_KEYS\n",
    "\n",
    "# Model objective function builder\n",
    "from hyperopt_objective import build_objective_func\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"paper\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import and pre-processing of dataset\n",
    "(preprocessing : transforming data into ML model readable format)\n",
    "\n",
    "#### Data Importing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data internally\n",
    "_df_train = pd.read_csv(\"DataFiles/CreditCard_train.csv\", index_col=0, header=1)\n",
    "_df_test = pd.read_csv(\"DataFiles/CreditCard_test.csv\", index_col=0, header=1)\n",
    "\n",
    "# create external df for handling\n",
    "df_train = _df_train.copy()\n",
    "df_test = _df_test.copy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data head and description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas DataFrame processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# renaming columns for consistency and simplicity\n",
    "df_train = df_train.rename(columns={'PAY_0':'PAY_1', 'default payment next month':'DEFAULT'})\n",
    "df_test = df_test.rename(columns={'PAY_0':'PAY_1', 'default payment next month':'DEFAULT'})\n",
    "\n",
    "label = df_train.columns[-1] # = `DEFAULT`\n",
    "features = list(df_train.columns)[:-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Formatting X, y as `np.ndarray`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train[label].to_numpy()\n",
    "X_train = df_train[features]\n",
    "\n",
    "y_test = df_test[label].to_numpy()\n",
    "X_test = df_test[features]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Comment__ : All the data types are integers and thus workable for ML models. There are no null values (arbitrarily checked and all features have the same count). Values in the `SEX`, `EDUCATION` are have a specified range, however, some values are not contained.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pipeline\n",
    "* includes scaling, sampling and (future work : feature transformation)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling the dataset for computational efficiency of ML models operations (fitting, prediction)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exporting training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "\n",
    "with open('train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, y_train, f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Benchmarking some standard ML models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking the ML models\n",
    "* adaboostingregressor, logistic regression and support vector machines\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter tuning of ML models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving data in a pickle file and opened again in model objectives. (not sure if I can include them as input to the objectives of each model, for version 2..)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameter tuning framework consists of a tuner (hyperopt), optimization space (model dependent), and objective function (model  dependent)\n",
    "These are imported.\n",
    "\n",
    "### ML models to be optimized\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuning\n",
    "\n",
    "For tuning we will be first split up the training data into a validation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_hyperopt_fitted_model(algorithmn, X_train : np.ndarray, y_train : np.ndarray, hyperparameters_space : dict,\n",
    "                                hyperparameters_choices : dict,  tuning_method : 'str' = 'train_validation_split',\n",
    "                                tuning_value : float = 0.25, tuning_measure = 'accuracy tuning', max_evals : int = 1,\n",
    "                                random_state : int = 0, **fitting_setting):\n",
    "    \"\"\"\n",
    "    Fits a ML model on the training data with hyperparameters tuned by Hyperopt using a selected cross validation\n",
    "    method.\n",
    "\n",
    "    :param algorithmn: ML algorithm for building model\n",
    "    :param hyperparameters_space: possible hyperparameters values\n",
    "    :param hyperparameters_choices: dictionary with 'key' as the choice and 'values' as options\n",
    "    :param X_train: X training data\n",
    "    :param y_train: X training data labels\n",
    "    :param tuning_method: method for tuning hyerparameters, either 'train_validation_split' or 'KFold'\n",
    "    :param tuning_value: tuning method value\n",
    "    :param tuning_measure: measure of score/performance on the validation set, either 'roc' or 'accuracy'\n",
    "    :param max_evals: maximum number of evaluations for hyperparameters searching\n",
    "    :param random_state: random_state\n",
    "    :param fitting_setting: additional settings for fitting the model (e.g. sample weight)\n",
    "\n",
    "    :return: fitted ML model\n",
    "    \"\"\"\n",
    "\n",
    "    # create trail\n",
    "    trials = Trials()\n",
    "\n",
    "    tuning_objective = build_objective_func(algorithmn=algorithmn, X_train=X_train, y_train=y_train,\n",
    "                                            tuning_method=tuning_method, tuning_value=tuning_value,\n",
    "                                            tuning_measure=tuning_measure, random_state=random_state, **fitting_setting)\n",
    "\n",
    "    print(f'Starting hyperparameter search with with {algorithmn}, {tuning_method}, {tuning_value}\\n')\n",
    "    model_best_hyperparams = fmin(fn = tuning_objective,\n",
    "                            space = hyperparameters_space,\n",
    "                            algo = tpe.suggest,\n",
    "                            max_evals = max_evals,\n",
    "                            trials = trials)\n",
    "\n",
    "    ## in case hyperparameters must be integers (e.g. max_depth for decision trees)\n",
    "    # fmin returns space in floats (for some reason changed the type int to float)\n",
    "    if type(INT_KEYS) == list:\n",
    "        for key in INT_KEYS:\n",
    "            if key in model_best_hyperparams.keys():\n",
    "                model_best_hyperparams[key] = int(model_best_hyperparams[key])\n",
    "\n",
    "    # when model space is a choice, `model_best_hyperparams` gives the index of the selected choice\n",
    "    # model options a dict with keys: as the option name and value as list of options\n",
    "    if type(hyperparameters_choices) == dict:\n",
    "        for choice, choice_option in hyperparameters_choices.items():\n",
    "            if choice in model_best_hyperparams.keys(): # additional safety net\n",
    "                model_best_hyperparams[choice] = choice_option[model_best_hyperparams[choice]]\n",
    "\n",
    "    model = algorithmn(**model_best_hyperparams) # refering to the algorithmn with hyperparameters as model\n",
    "    model.fit(X_train, y_train, **fitting_setting)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance at given percentages\n",
    "### robustness\n",
    "\n",
    "As opposed to simply classifiying clients as expected to default vs not-expected to default, quantifying is more meaningful. I.e. defining a probability of default has more potential.\n",
    "\n",
    "To estimate the real probability, the Smooth Sorting Method can be used, which estimates the real probability by looking at neighboring points and taking the mean of these values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Smooth Sorting Method__ from the original paper (Yeh, I. C., & Lien, C. H. (2009)):\n",
    "\n",
    "$$\\text{P}_i = \\frac{\\sum_{j=-n}^{n}\\text{Y}_{i-j}}{2n+1}$$\n",
    "\n",
    "where $\\text{P}_i$ is the estimated real probability of default, $\\text{Y}_{i}$ is the binary variable of default (1) or non-default (0), $n$ is the number of data for smoothing.<br>\n",
    "The Smooth Sorting Method is used on sorted data, from the lowest probability of default occuring to the highest probability of default occuring.\n",
    "\n",
    "This is interesting to look at because loaners adopt different risk strategies.\n",
    "( (?) for this we consider at 20% and 80% (?) )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def SSM_real_probability(y_real : np.ndarray, y_predicted : np.ndarray, n : int, plot : bool = True):\n",
    "\n",
    "    sorted_index = np.argsort(y_predicted)\n",
    "    y_real_sorted = y_real[sorted_index]\n",
    "    y_predicted_sorted = y_predicted[sorted_index]\n",
    "\n",
    "    intermediate_real_probability = np.array([])\n",
    "    for counter in range(n, len(y_real)-n):\n",
    "        intermediate_real_probability = np.append(intermediate_real_probability,\n",
    "                                                  np.mean(y_real_sorted[counter-n:counter+n]))\n",
    "\n",
    "    r2 = r2_score(y_real, y_predicted)\n",
    "\n",
    "    if plot:\n",
    "        y_predicted_selected = y_predicted_sorted[n:len(y_real)-n]\n",
    "        plt.plot(y_predicted_selected,intermediate_real_probability)\n",
    "        plt.grid(True)\n",
    "        plt.xlim([0,1])\n",
    "        plt.xlabel('Predicted probability')\n",
    "        plt.ylabel('Real probability using SSM')\n",
    "        plt.annotate(f'$R^2 = {r2}$', (0.05, 0.95))\n",
    "        plt.grid(True, which='both')\n",
    "        plt.show()\n",
    "\n",
    "    print(f'r2 score : {r2}\\n')\n",
    "    return r2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "algorithmns_list = [AdaBoostRegressor,\n",
    "                    LogisticRegression,\n",
    "                    xgb.XGBRegressor,\n",
    "                    SVC\n",
    "                    ]\n",
    "\n",
    "# imported\n",
    "hyperparameter_space_list = [ada_space,\n",
    "                             log_space,\n",
    "                             xgb_space,\n",
    "                             svm_space\n",
    "                             ]\n",
    "\n",
    "hyperparameter_choices_list = [{'loss' : ada_loss_functions},\n",
    "                               None,\n",
    "                               None,\n",
    "                               {'kernel' : svm_kernels, 'degree' : svm_kernel_degrees}\n",
    "                               ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dictionary ordering\n",
    "model_name (e.g. `ada`) -> tuning_method (`train_validation_split`, `train_validation_split_randomized`, `KFold`) -> tuning_value -> tuning_measure -> max_evals -> score (accuracy, roc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tuning_measures_list = ['accuracy tuning', 'roc auc tuning']\n",
    "tuning_measures_list = ['accuracy tuning']\n",
    "tuning_methods_dict = {'train_validation_split' : {}, 'KFold' : {}}\n",
    "\n",
    "# model performance dictionary building\n",
    "model_ref_names = ['ada', 'log', 'xgb', 'svm']\n",
    "selected_models = [model_ref_names[0]]\n",
    "MODELS_PERFORMANCES = {}\n",
    "for model_name in selected_models:\n",
    "    MODELS_PERFORMANCES[model_name] = copy.deepcopy(tuning_methods_dict)\n",
    "\n",
    "print(MODELS_PERFORMANCES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set variable `long_run=True` for a long run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# intialization\n",
    "ratio_range = range(20,30+1,5)\n",
    "KFold_range = [4]\n",
    "max_evals_range = range(25,35+1,10)\n",
    "random_state = 0\n",
    "n = 50\n",
    "\n",
    "max_evals_range = range(30, 31, 10)\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    max_evals_range = range(1,1+1,1)\n",
    "    ratio_range = range(20,20+1,5)\n",
    "\n",
    "tuning_value_range_list = [ratio_range, KFold_range]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "tuning_measure_performance = {}\n",
    "performance_at_num_iterations = {}\n",
    "\n",
    "\n",
    "for algorithmn, hyperparameters_space, hyperparameters_choices, model_name in zip(algorithmns_list,\n",
    "                                                                                  hyperparameter_space_list,\n",
    "                                                                                         hyperparameter_choices_list,\n",
    "                                                                                      MODELS_PERFORMANCES):\n",
    "    for tuning_method_iterator, tuning_value_range in zip(tuning_methods_dict, tuning_value_range_list):\n",
    "        for tuning_value_iterator in tuning_value_range:\n",
    "            if tuning_method_iterator == 'train_validation_split' or tuning_value_iterator == 'train_validation_split_randomized':\n",
    "                tuning_value_iterator = tuning_value_iterator/100 # setting as a decimal\n",
    "            for max_evals_iterator in max_evals_range:\n",
    "                for tuning_measure_iterator in tuning_measures_list:\n",
    "                    model = build_hyperopt_fitted_model(algorithmn=algorithmn, X_train= X_train, y_train= y_train,\n",
    "                                                        hyperparameters_space = hyperparameters_space,\n",
    "                                                        hyperparameters_choices = hyperparameters_choices,\n",
    "                                                        tuning_method = tuning_method_iterator,\n",
    "                                                        tuning_measure = tuning_measure_iterator, tuning_value = tuning_value_iterator,\n",
    "                                                        max_evals = max_evals_iterator,\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                    performance = {'test accuracy': performance_metrics(y_test, y_pred>0.5, confusion_matrix=False),\n",
    "                                   'test roc auc': roc_auc_score(y_test,y_pred, average = 'macro'),\n",
    "                                   'test r2': SSM_real_probability(y_test,y_pred, n=n, plot=False)}\n",
    "\n",
    "                    performance_at_num_iterations[max_evals_iterator] = [copy.deepcopy(performance)]\n",
    "                    tuning_measure_performance[tuning_measure_iterator] = copy.deepcopy(performance_at_num_iterations)\n",
    "\n",
    "                    # tuning_measure_performance[tuning_measure_iterator] = copy.deepcopy(performance)\n",
    "                    assert type(tuning_measure_iterator) == str\n",
    "                MODELS_PERFORMANCES[model_name][tuning_method_iterator][tuning_value_iterator] = copy.deepcopy(tuning_measure_performance)\n",
    "\n",
    "# can stop the evaluation at 20 times and save parameters, this almost halves the simulation time. log model hyperparameters also don't change\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODELS_PERFORMANCES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('results2.pkl', 'wb') as f:\n",
    "    pickle.dump(MODELS_PERFORMANCES, f)\n",
    "\n",
    "print(MODELS_PERFORMANCES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model performance of ada : `MODELS_PERFORMANCES['ada']['train_split_ratio']`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting the performance on accuracy under different tuning strategies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def select_plot_data(tuning_method_list : list, tuning_method_value : float, max_evals : int = 25, performance_score : str = 'test accuracy', tuning_measures_list : list = tuning_measures_list) -> tuple:\n",
    "    score_list = []\n",
    "    for tuning_method_iterator in tuning_method_list:\n",
    "        for tuning_measure_name in tuning_measures_list:\n",
    "            score_list.append(MODELS_PERFORMANCES[model_name][tuning_method_iterator][tuning_method_value][tuning_measure_name][max_evals][performance_score])\n",
    "\n",
    "    accuracy_tuned_scores = score_list[::2]\n",
    "    roc_auc_tuned_scores = score_list[1::2]\n",
    "\n",
    "    return accuracy_tuned_scores, roc_auc_tuned_scores\n",
    "\n",
    "\n",
    "\n",
    "ratio_0 = 0.25\n",
    "max_evals_0 = 25\n",
    "\n",
    "tuning_method_name = 'train_validation_split'\n",
    "tuning_measure_name = tuning_measures_list[0] # = 'accuracy tuning'\n",
    "\n",
    "performance_scores = ['test accuracy' , 'test roc auc', 'test r2']\n",
    "selected_performance_score = performance_scores[0]\n",
    "\n",
    "print(MODELS_PERFORMANCES['ada'][tuning_method_name][0.25][tuning_measure_name])\n",
    "\n",
    "selected_data = select_plot_data(tuning_method_list=list(tuning_methods_dict), tuning_method_value=ratio_0, max_evals=max_evals_0, performance_score=selected_performance_score)\n",
    "accuracy_tuned_score_list, roc_auc_tuned_score_list = selected_data\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.subplots_adjust(left=0.25, bottom=0.20)\n",
    "\n",
    "xticks_variables = list(tuning_methods_dict)\n",
    "\n",
    "# The x position of bars\n",
    "bar_width = 0.25\n",
    "r1 = np.arange(len(xticks_variables))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "positions = [r1, r2]\n",
    "\n",
    "bars1 = ax.bar(r1, accuracy_tuned_score_list, color ='b', width = bar_width, label='accuracy tuned')\n",
    "bars2 = ax.bar(r2, roc_auc_tuned_score_list, color ='g', width = bar_width, label='roc auc tuned')\n",
    "\n",
    "# bars2.remove()\n",
    "bars_list = [bars1, bars2]\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.ylim([0.7, 0.84])\n",
    "plt.xticks(r1+bar_width/2, xticks_variables)\n",
    "plt.xlabel('Tuning method')\n",
    "plt.ylabel(selected_performance_score)\n",
    "\n",
    "\n",
    "axcolor = 'lightgoldenrodyellow'\n",
    "ax_ratio = plt.axes([0.15, 0.2, 0.03, 0.675], facecolor=axcolor)\n",
    "s_ratio = Slider(ax=ax_ratio, label='ratio', valmin=ratio_range[0]/100, valmax=ratio_range[-1]/100, valinit=ratio_0, valstep=ratio_range.step/100, orientation='vertical')\n",
    "\n",
    "ax_max_evals = plt.axes([0.05, 0.2, 0.03, 0.675], facecolor=axcolor)\n",
    "s_max_evals = Slider(ax=ax_max_evals, label='max_evals', valmin=max_evals_range[0], valmax=max_evals_range[-1], valinit=max_evals_0, valstep=max_evals_range.step, orientation='vertical')\n",
    "\n",
    "def bar_plot(y_label, data1, data2):\n",
    "\n",
    "    for bars in bars_list:\n",
    "        bars.remove()\n",
    "\n",
    "    bars_list[0] = ax.bar(r1, data1, color='b', width=bar_width, label='accuracy tuned')\n",
    "    bars_list[1] = ax.bar(r2, data2, color='g', width=bar_width, label='roc auc tuned')\n",
    "\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "\n",
    "def update(val):\n",
    "    ratio = round(s_ratio.val*100)/100\n",
    "    max_evals = s_max_evals.val\n",
    "    updated_data = select_plot_data(tuning_method_list=list(tuning_methods_dict), tuning_method_value=ratio,  max_evals=max_evals, performance_score=selected_performance_score)\n",
    "    selected_accuracy_tuned_score_list, selected_roc_auc_tuned_score_list = updated_data\n",
    "    bar_plot(selected_performance_score, selected_accuracy_tuned_score_list, selected_roc_auc_tuned_score_list)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "s_ratio.on_changed(update)\n",
    "s_max_evals.on_changed(update)\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "pickle.dump(fig, open('train_val.fig.pickle', 'wb'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "roc_curve = True\n",
    "\n",
    "if roc_curve:\n",
    "    plot_roc_curve(model, X_test, y_test)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The search selects the best considered (not the best in the space) generalizable hyperparameters (i.e. the ones that perform best, after fitting on the training set and prediction on the validation set).\n",
    "In this limited analysis, these hyperparameters are considered the best generalizable.\n",
    "Alternatively, we can say that we select these parameters to be tested.\n",
    "We may find that some ML models allow for a large fluctuation in performance on the validation set, which may indicate that we have 'overfitted' the validation set. This will show on the test set.\n",
    "We can decide to further train on the validation set, however, it is interesting to see how the performance of the model changes whether we use the validation set or not.\n",
    "\n",
    "The models are:\n",
    "`xgb_reg`, `ada_reg`, `gbrt_reg`, `log_reg`, and `svm_reg`\n",
    "\n",
    "Now we can test these models on the test set. Followed by checking each prediction's 'strenght' by using the Smooth Sorting Method as proposed in the original paper by Yeh and Lien.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}